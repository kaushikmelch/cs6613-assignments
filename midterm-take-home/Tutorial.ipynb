{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TUTUORIAL:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial file aims to explain the usage of the code used to train the CNN using CIFAR10 and to get the information plane and the plot gradients replicating the work of Tishby et al."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1) This code is a modified version of the IDNNs repo provided. The code can be executed by runnning the main.py file similar to the original code.\n",
        "2) The code initializes a network using the constructor in information_network.py. This constructors sets some default parameters using network_parameters.py. These include setting save_grads = True and cov_net as True which ensures that the gradients are saved and the model that will be built is a convolutional neural network.\n",
        "3) The network is then run which builds a CNN model of two convolutional layers and two max pooling layers followed by two fully connected layers and a dropout layer. The code of this CNN is present in models.py in the functions deepnn().\n",
        "4) After the CNN is built. It is trained for 8000 epochs with a batch size of 64. \n",
        "Note: In my case, due to restrictions on hardware, I was able to run only 7 epochs such that the information plane was generatable for the scenario.\n",
        "5) After the training is completed, the information is calculated.\n",
        "The following process is followed in the information calculation. This is available in information_process.py\n",
        "6) Preparation: Before calculating information measures, the function prepares the necessary data and parameters. This includes defining bins for discretizing data, extracting probabilities from labels and inputs.\n",
        "7) Information Calculation: The main computation happens inside a loop over the epochs of training. For each epoch, information measures are calculated for each layer of the network. This involves passing various parameters such as weight matrices (ws), input data (x), labels (label), the number of bins, and model-related information.\n",
        "8) Return: The function returns a collection of information measures for each layer across all epochs. These measures include mutual information and entropy quantities, providing insights into the information processing capabilities of the network at different stages of training.\n",
        "9) After the information is calculated, an information plane plot is generated that plots mutual information of X and Inner Layers vs mutual information of Y and Inner Layers.\n",
        "10) The next plot includes the gradient mean and variance of each layer as the epochs continue.\n",
        "Note- I couldnt plot the gradient mean and variance graphs due to hardware restrictions as the memory overflowed and killed the process.\n",
        "\n",
        "\n",
        "Plots included in the repository\n",
        "1) 1B-existing_code.jpg : This plot is a replication of the information plane shown by Tisby et al using the existing codebase of IDNNs that uses a Deep neural net for a binary classification task.\n",
        "2) 1B-CNN_Info_Plane.jpg : This plot is the information plane generated using the CNN for 7 epochs(due to hardware restrictions)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
